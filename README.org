#+BEGIN_CENTER
/Because Caesar.jl was taken/ 
#+END_CENTER

* Gaius.jl
Gaius.jl is a multi-threaded BLAS-like library using a
divide-and-conquer strategy to parallelism, and built on top of the
*fantastic* [[https://github.com/chriselrod/LoopVectorization.jl][LoopVectorization.jl]]. Gaius.jl spawns threads using
Julia's depth first parallel task runtime and so Gaius.jl's routines
may be fearlessly nested inside multi-threaded julia programs.

** Matrix Multiplication
Currently, fast, native matrix-multiplication is only implemented
between matrices of types ~Matrix{<:Union{Float64, Float32, Int64,
Int32}}~, though I have plans to provide support for ~Complex~
numbers as well as other commonly encountered numeric ~struct~ types
such as ~Dual~ numbers.

*** Using Gaius.jl 
Gaius.jl does not export any functions, but it internally defines
~Gaius.:(*)~ and ~Gaius.mul!~ which do out-of-place and in-place
matrix multiplication on matrices of type ~Matrix{<:Union{Float64,
Float32, Int64, Int32}}~ respectively.

~Gaius.:(*)~ and ~Gaius.mul!~ are *not* equal to ~Base.:(*)~ and
~LinearAlgebra.mul!~ though ~Gaius~'s definitions do fall back on
those definitions for types other than those specialized matrix types.

If you do
#+BEGIN_SRC julia
using Gaius: *, mul! 
#+END_SRC
in a scope before using ~*~ or ~mul!~ in that scope, then ~Gaius.jl~'s
definitions will /shadow/ the regular ones. This is not type piracy
and these shadowed definitions will not propagate into enclosing scopes.

#+BEGIN_SRC julia
julia> using Gaius, BenchmarkTools, LinearAlgebra

julia> A, B, C = rand(104, 104), rand(104, 104), zeros(104, 104);

julia> @btime mul!($C, $A, $B);
  42.364 μs (0 allocations: 0 bytes)

julia> @btime Gaius.mul!($C, $A, $B);
  30.064 μs (131 allocations: 4.53 KiB)
#+END_SRC


#+BEGIN_SRC julia
julia> using Gaius, BenchmarkTools

julia> A, B = rand(104, 104), rand(104, 104);

julia> @btime $A * $B;
  45.657 μs (2 allocations: 84.58 KiB)

julia> @btime let * = Gaius.:(*)
           $A * $B
       end;
  30.875 μs (133 allocations: 89.11 KiB)
#+END_SRC

*** Benchmarks 
**** Floating Point Performance 
The following benchmarks were run on this 
#+BEGIN_SRC julia
julia> versioninfo()
Julia Version 1.4.0-rc2.0
Commit b99ed72c95* (2020-02-24 16:51 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: AMD Ryzen 5 2600 Six-Core Processor
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-8.0.1 (ORCJIT, znver1)
Environment:
  JULIA_NUM_THREADS = 6
#+END_SRC
and compared to [[https://github.com/xianyi/OpenBLAS][OpenBLAS]] running with ~6~ threads
(~BLAS.set_num_threads(6)~). I would be keenly interested in seeing
analogous benchmarks on a machine with an AVX512 instruction set and / or [[https://software.intel.com/en-us/mkl][Intel's MKL]].

[[file:assets/F64_mul.png]]

[[file:assets/F32_mul.png]]

#+BEGIN_CENTER
/Note that these are log-log plots/ 
#+END_CENTER

Gaius.jl outperforms [[https://github.com/xianyi/OpenBLAS][OpenBLAS]] over a very wide useful range of matrix
sizes, but does eventually start falling behind when it approaches =~600 × ~600= matrices.

**** Integer Performance
These benchmarks compare Gaius.jl (on the same machine as above) and
compare against julia's generic matrix multiplication implementation
(OpenBLAS does not provide integer mat-mul) which is not
multi-threaded. I would be keenly interested in seeing analogous
benchmarks on a machine with an AVX512 instruction set.

[[file:assets/I64_mul.png]]

[[file:assets/I32_mul.png]]

#+BEGIN_CENTER
/Note that these are log-log plots/ 
#+END_CENTER

If you find yourself in a high performance situation where you want to
multiply matrices of integers, I think this provides a compelling
use-case for Gaius.jl since it will outperform it's competition at
*any* matrix size and for large matrices will benefit from
multi-threading.


** Other BLAS Routines
I have not yet worked on implementing other standard BLAS routines
with this strategy, but doing so should be relatively straightforward.
