#+BEGIN_CENTER
/Because Caesar.jl was taken/ 
#+END_CENTER

* Gaius.jl
Gaius.jl is a multi-threaded BLAS-like library using a
divide-and-conquer strategy to parallelism, and built on top of the
*fantastic* [[https://github.com/chriselrod/LoopVectorization.jl][LoopVectorization.jl]]. Gaius.jl spawns threads using
Julia's depth first parallel task runtime and so Gaius.jl's routines
may be fearlessly nested inside multi-threaded julia programs.

Gaius is currently faster than OpenBLAS for all matrix sizes I've
managed to test and I think there are still a lot of performance gains
to be had.

** Matrix Multiplication
Currently, fast, native matrix-multiplication is only implemented
between matrices of types ~Matrix{<:Union{Float64, Float32, Int64,
Int32}}~, though I have plans to provide support for ~Complex~
numbers as well as other commonly encountered numeric ~struct~ types
such as ~Rational~ and ~Dual~ numbers.

*** Using Gaius.jl 
Gaius.jl does not export any functions, but it internally defines
~Gaius.:(*)~ and ~Gaius.mul!~ which do out-of-place and in-place
matrix multiplication on matrices of type ~Matrix{<:Union{Float64,
Float32, Int64, Int32}}~ respectively.

~Gaius.:(*)~ and ~Gaius.mul!~ are *not* equal to ~Base.:(*)~ and
~LinearAlgebra.mul!~ though ~Gaius~'s definitions do fall back on
those definitions for types other than those specialized matrix types.

If you do
#+BEGIN_SRC julia
using Gaius: *, mul! 
#+END_SRC
in a scope before using ~*~ or ~mul!~ in that scope, then ~Gaius.jl~'s
definitions will /shadow/ the regular ones. This is not type piracy
and these shadowed definitions will not propagate into enclosing scopes.

#+BEGIN_SRC julia
julia> using Gaius, BenchmarkTools, LinearAlgebra

julia> A, B, C = rand(104, 104), rand(104, 104), zeros(104, 104);

julia> @btime mul!($C, $A, $B);
  42.364 μs (0 allocations: 0 bytes)

julia> @btime Gaius.mul!($C, $A, $B);
  30.064 μs (131 allocations: 4.53 KiB)
#+END_SRC

You can change the matrix sub-block size by calling ~mul!~ with the
~size_cutoff~ keyword argument, the default to which is ~64÷2~, i.e. a
matrix will be split into subblocks along any dimension exceeding 64
entries (I'll change this interface to be a bit less confusing!)

#+BEGIN_SRC julia
julia> using Gaius, BenchmarkTools

julia> A, B = rand(104, 104), rand(104, 104);

julia> @btime $A * $B;
  45.657 μs (2 allocations: 84.58 KiB)

julia> @btime let * = Gaius.:(*)
           $A * $B
       end;
  30.875 μs (133 allocations: 89.11 KiB)
#+END_SRC

*** Benchmarks 
**** Floating Point Performance 
The following benchmarks were run on this 
#+BEGIN_SRC julia
julia> versioninfo()
Julia Version 1.4.0-rc2.0
Commit b99ed72c95* (2020-02-24 16:51 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: AMD Ryzen 5 2600 Six-Core Processor
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-8.0.1 (ORCJIT, znver1)
Environment:
  JULIA_NUM_THREADS = 6
#+END_SRC
and compared to [[https://github.com/xianyi/OpenBLAS][OpenBLAS]] running with ~6~ threads
(~BLAS.set_num_threads(6)~). I would be keenly interested in seeing
analogous benchmarks on a machine with an AVX512 instruction set and / or [[https://software.intel.com/en-us/mkl][Intel's MKL]].

[[file:assets/F64_mul.png]]

[[file:assets/F32_mul.png]]

#+BEGIN_CENTER
/Note that these are log-log plots/ 
#+END_CENTER

Gaius.jl outperforms [[https://github.com/xianyi/OpenBLAS][OpenBLAS]] for as high up in matrix sizes as I've bothered to test it. The larest I've tested is ~10,000 x 10,000~ arrays at which Gaius beats OpenBLAS by a factor of 50 runtime speed.

**** Integer Performance
These benchmarks compare Gaius.jl (on the same machine as above) and
compare against julia's generic matrix multiplication implementation
(OpenBLAS does not provide integer mat-mul) which is not
multi-threaded. I would be keenly interested in seeing analogous
benchmarks on a machine with an AVX512 instruction set.

Graphs coming soon
#+BEGIN_COMMENT
[[file:assets/I64_mul.png]]

[[file:assets/I32_mul.png]]
#+END_COMMENT


#+BEGIN_CENTER
/Note that these are log-log plots/ 
#+END_CENTER

If you find yourself in a high performance situation where you want to
multiply matrices of integers, I think this provides a compelling
use-case for Gaius.jl since it will outperform it's competition at
*any* matrix size and for large matrices will benefit from
multi-threading.


** Other BLAS Routines
I have not yet worked on implementing other standard BLAS routines
with this strategy, but doing so should be relatively straightforward.

** Safety
/If you must break the law, do it to seize power; in all other cases observe it./

    -Gaius Julius Caesar

If you use only the functions ~Gaius.mul!~ and ~Gaius.:(*)~, automatic
array size-checking will occur before the matrix multiplication
begins. This can be turned off in ~mul!~ by calling ~Gaius.mul!(C, A,
B, sizecheck=false)~, in which case no sizechecks will occur on the
arrays before the matrix multiplication occurs and all sorts of bad,
segfaulty things can happen.

All other functions in this package are to be considered /internal/
and should not be expected to check for safety or obey the law. The
functions ~Gaius.gemm_kernel!~ and ~Gaius.add_gemm_kernel!~ may be of
utility, but be warned that they do not check array sizes.
